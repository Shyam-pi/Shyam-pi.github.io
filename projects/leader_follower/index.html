<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Leader-Follower using TerpBot* | Shyamsundar Prabhakar Indra</title> <meta name="author" content="Shyamsundar Prabhakar Indra"> <meta name="description" content="TerpBot following a human leader along with dynamic obstacle avoidance using a synthetic 2D point cloud generated with MiDaS Monocular Depth Estimation"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shyam-pi.github.io/projects/leader_follower/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shyamsundar </span>Prabhakar Indra</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/personal/">Personal</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Leader-Follower using TerpBot*</h1> <p class="post-description">TerpBot following a human leader along with dynamic obstacle avoidance using a synthetic 2D point cloud generated with MiDaS Monocular Depth Estimation</p> </header> <article> <p><strong>Note:</strong> This project was done in collaboration with <a href="https://github.com/vedran97" rel="external nofollow noopener" target="_blank">Vedant Ranade</a> and <a href="https://github.com/SakshamV" rel="external nofollow noopener" target="_blank">Saksham Verma</a>. <strong>DEMO</strong></p> <p>Click on the following image to get redirected to a demo video of the project. It showcases how the TerpBot trying to detect and follow a human leader while avoiding any dynamic obstacles in its path, detected using a combination of YOLO and MiDaS monocular depth estimation models.</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <a href="https://www.youtube.com/watch?v=kOSTu5ZXVmQ&amp;ab_channel=Shyam" rel="external nofollow noopener" target="_blank"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/rrt_star/demo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/rrt_star/demo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/rrt_star/demo-1400.webp"></source> <img src="/imgs/rrt_star/demo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="leader-follower Demo" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </a> </div> </div> <div class="caption"> Demo showcasing Leader-Follower capabilities of TerpBot. Click on the above image to open the video. </div> <p><strong>GOAL</strong></p> <p>The idea of a leader-follower system draws inspiration from natural phenomena, such as flocking behavior observed in birds and schooling behavior observed in fish. These natural systems exhibit remarkable coordination and adaptability, where individuals follow a leader while avoiding obstacles and maintaining group cohesion. By emulating these principles in robotics, we can create intelligent machines capable of robust and efficient tracking of dynamic targets.</p> <p>In this project we enable the TerpBot to become a follower robot that can detect and follow humans who are the leader entities. Taking inspiration from Tesla which predominantly relies on only cameras for perception, the TerpBot uses only a monocular camera (RaspiCamV2) to localize the position of the leader object with respect to the TerpBot and following it. We also enable it to avoid collisions with dynamic obstacles introduced in the environment. This involves setting up the perception stack of TerpBot, which can successfully localize the leader object, as well as create a point cloud of the environment using only a monocular camera. We also formulate a simple control algorithm to make TerpBot follow the leader while avoiding collisions with the dynamic obstacles.</p> <p><strong>METHOD - Perception</strong></p> <p>The video feed from the Raspicamv2 is read by the software system using ROS cv bridge. Each frame of the feed is then processed through a custom ROS node consisting of two deep learning models, which attempt to detect and localize the leader (humans), with respect to the TerpBot. The two deep learning models are as follows:</p> <ul> <li>Object Detection model - Gives a 2D bounding box around the leader object present in the video frame.</li> <li>Monocular Depth Estimation model - Gives a pixel-wise depth map using a monocular image as the input. The bounding box from the first model is retrieved and used in the depth map to get the distance of the leader object from the camera.</li> </ul> <p>The algorithm and architecture of both models can be found in the following sections. A simple representation of this perception pipeline is as follows :</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/perception_pipeline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/perception_pipeline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/perception_pipeline-1400.webp"></source> <img src="/imgs/leader_follower/perception_pipeline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Perception Pipeline" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> A high level overview of the two model perception stack of TerpBot for Leader-Follower task </div> <p><strong>YOLO Object Detection Model</strong></p> <p>The object detection model that the TerpBot uses is based on the 5th iteration of the famous YOLO object detection algorithm, as detailed in the paper “You Only Look Once: Unified, Real-Time Object Detection”. The base model is trained on the COCO dataset, which comprises 80 different classes of objects. The model takes a single image as input and provides a set of bounding boxes enclosing different objects present in the image as the output, along with their class names. A pre-trained YOLOv5 model was taken and trained using a dataset which detects humans only using the lower parts of their body (since the camera’s FOV can’t capture entire humans). A summary of the YOLO model can be found below :</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/yolov5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/yolov5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/yolov5-1400.webp"></source> <img src="/imgs/leader_follower/yolov5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="YOLO summary" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Summary of the YOLOv5 algorithm and architecture </div> <p>The following loss functions are used to train the YOLOv5 model:</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/yolov5_losses-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/yolov5_losses-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/yolov5_losses-1400.webp"></source> <img src="/imgs/leader_follower/yolov5_losses.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="YOLO losses" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Loss functions used to train the YOLOv5 model </div> <p>The below image shows sample results from the YOLOv5 model used on TerpBot:</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/yolov5_results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/yolov5_results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/yolov5_results-1400.webp"></source> <img src="/imgs/leader_follower/yolov5_results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="YOLO results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Sample results from TerpBot's YOLOv5 model on detecting the Leader human. </div> <p><strong>MiDaS Monocular Depth Estimation Model</strong></p> <p>The Depth Estimation technique used by TerpBot is the ‘MiDaS (Monocular Depth Estimation in Real-time with Adaptive Scale)’ model, based on the paper ‘Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer’. This model was chosen for the depth estimation task for our robot due to its exceptional generalizability. It has been trained on a diverse range of data across various task domains, making it state-of-the-art in depth estimation and a pre-trained model was used directly. Moreover, it demonstrates robust performance even on images that do not belong to the training distribution. A summary of the MiDaS model can be found below :</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/midas-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/midas-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/midas-1400.webp"></source> <img src="/imgs/leader_follower/midas.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="midas summary" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Summary of the MiDaS algorithm and architecture </div> <p>The following loss functions are used to train the MiDaS model:</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/midas_losses-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/midas_losses-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/midas_losses-1400.webp"></source> <img src="/imgs/leader_follower/midas_losses.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="midas losses" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Loss functions used to train the MiDaS model </div> <p>The below image shows sample results from the MiDaS model used on TerpBot:</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/midas_results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/midas_results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/midas_results-1400.webp"></source> <img src="/imgs/leader_follower/midas_results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="midas results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Sample results from TerpBot's MiDaS model to form a 2D point cloud. </div> <p><strong>OVERALL PIPELINE</strong></p> <p>The below image is a schematic representation of the overall pipeline used by the TerpBot to carry out the Leader-Follower task successfully:</p> <div class="row justify-content-center"> <div class="col-sm-auto mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/imgs/leader_follower/overall_pipeline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/imgs/leader_follower/overall_pipeline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/imgs/leader_follower/overall_pipeline-1400.webp"></source> <img src="/imgs/leader_follower/overall_pipeline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Overall Pipeline" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall pipeline utilized by the TerpBot for Leader-Follower task </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Shyamsundar Prabhakar Indra. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>